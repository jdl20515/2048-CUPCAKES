{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdl20515/2048-CUPCAKES/blob/gh-pages/Investigating_the_Effectiveness_of_Convolutional_Neural_Networks_on_Retinal_Disease_Diagnosis_Using_the_MURED_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the model that was used in the paper *Investigating the Effectiveness of Convolutional Neural Networks on Retinal Disease Diagnosis Using the MURED Dataset*, published in the 2024 IEEE IATMSI conference.\n",
        "\n",
        "The full paper can be found at IEEE Explore: https://ieeexplore.ieee.org/document/10502464.\n",
        "\n",
        "Jose David Lomelin; An Tran; Saumik Das; Lakshmisaketh Alluri; Rushil Challa; Sahil Sanjeev Narula; Aaditiya Jaganathan"
      ],
      "metadata": {
        "id": "EDEFl1T1zaCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFS-nYrQOedo",
        "outputId": "8ced23f3-3477-43b5-e980-0625fbe2afb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-ranking\n",
            "  Downloading tensorflow_ranking-0.5.2-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.4/150.4 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
            "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-ranking) (1.15.0)\n",
            "Collecting numpy==1.23.2\n",
            "  Downloading numpy-1.23.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-ranking) (1.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.51.1)\n",
            "Requirement already satisfied: tensorflow<3,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.30.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (23.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (23.1.21)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (15.0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.13.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.2.2)\n",
            "Installing collected packages: numpy, tensorflow-serving-api, tensorflow-ranking\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.2 tensorflow-ranking-0.5.2 tensorflow-serving-api-2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow_addons\n",
        "!pip install tensorflow-ranking\n",
        "#pip install pyyaml h5py\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_ranking as tfr\n",
        "import tensorflow_addons as tfa\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import keras.api._v2.keras as keras\n",
        "import random\n",
        "\n",
        "from tensorflow.keras import Model, Input, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, Flatten, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2M, preprocess_input\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.applications.resnet_v2  import ResNet50V2, preprocess_input\n",
        "from keras.layers import Dense, Flatten, Concatenate\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "\n",
        "# Run If Vgg16 does not work\n",
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==2.9.1\n",
        "\n",
        "# Run if tfr/tfad does not work\n",
        "#!pip install tensorflow-ranking\n",
        "#!pip install tensorflow_addons\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKWycWElOrc3",
        "outputId": "be943bb5-bc69-4f73-acb7-4c643fd41f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1444: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2029 images belonging to 19 classes.\n",
            "Found 523 images belonging to 19 classes.\n",
            "(1, 384, 384, 3)\n"
          ]
        }
      ],
      "source": [
        "# Loads image in from the set image path\n",
        "#from PIL import\n",
        "# Mount and Set Dataset Path (MURED)\n",
        "# Can vary from 520x520 to 3400x2800\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "INPUT = (384, 384, )\n",
        "\n",
        "#Define Noise Function\n",
        "def add_noise(img):\n",
        "    '''Add random noise to an image'''\n",
        "    VARIABILITY = 6\n",
        "\n",
        "    #Add noise only 20% of the time\n",
        "    rand = random.random()\n",
        "    if rand > .8:\n",
        "      deviation = VARIABILITY*random.random()\n",
        "      noise = np.random.normal(0, deviation, img.shape)\n",
        "      img += noise\n",
        "      np.clip(img, 0., 255.)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Add Data Augmentations Dataset\n",
        "train = ImageDataGenerator(rescale = 1/255, width_shift_range=[-10, 10], height_shift_range=[-10, 10], brightness_range=[0.8, 1.2], horizontal_flip = True, zoom_range=.1, samplewise_center=True, samplewise_std_normalization=True, zca_whitening=True, preprocessing_function=add_noise,)\n",
        "val = ImageDataGenerator(rescale = 1/255, width_shift_range=[-10, 10], height_shift_range=[-10, 10], brightness_range=[0.8, 1.2], horizontal_flip = True, zoom_range=.1, samplewise_center=True, samplewise_std_normalization=True, zca_whitening=True, preprocessing_function=add_noise,)\n",
        "sample = ImageDataGenerator(rescale = 1/255)\n",
        "\n",
        "\n",
        "#Define Training and Validation Set\n",
        "training_set = train.flow_from_directory(\"/content/drive/MyDrive/MURED/train\",\n",
        "                                         target_size = INPUT,\n",
        "                                         batch_size = 32,\n",
        "                                         class_mode = \"categorical\",\n",
        "                                         )\n",
        "\n",
        "validation_set = val.flow_from_directory(\"/content/drive/MyDrive/MURED/val\",\n",
        "                                         target_size = INPUT,\n",
        "                                         batch_size = 32,\n",
        "                                         class_mode = \"categorical\",\n",
        "\n",
        "                                         )\n",
        "\n",
        "#Fit Generators\n",
        "from numpy import asarray\n",
        "from PIL import Image\n",
        "spath = Image.open(\"/content/drive/MyDrive/MURED/small_sample/sample/23.png\")\n",
        "spath = spath.resize((384, 384))\n",
        "simage = asarray(spath)\n",
        "simage = simage.astype('float32')\n",
        "\n",
        "\n",
        "simage = np.expand_dims(simage, axis=0)\n",
        "print(simage.shape)\n",
        "\n",
        "#train.fit(simage)\n",
        "#val.fit(simage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVB8-tAvOu57",
        "outputId": "0110b946-2656-4b91-b020-d850fd008980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94668760/94668760 [==============================] - 1s 0us/step\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 6, 6, 8192), dtype=tf.float32, name=None), name='max_pooling2d_7/MaxPool:0', description=\"created by layer 'max_pooling2d_7'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 6, 6, 4096), dtype=tf.float32, name=None), name='conv2d_12/Relu:0', description=\"created by layer 'conv2d_12'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 147456), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n"
          ]
        }
      ],
      "source": [
        "# Make Model\n",
        "\n",
        "# Make a copy of this model, with original idea (run xception, then resize with conv layers to fit the original image size, then pass into rest of models. Compare to this model's efficiency/time)\n",
        "# DONT FORGET TO FREEZE NON FC LAYERS\n",
        "# DONT FORGET TO DO CHECKPOINTS\n",
        "\n",
        "INPUT = (384, 384, 3)\n",
        "inputlayer = Input(shape=INPUT)\n",
        "\n",
        "# Initializing Pretrained Models and Setting Convolutions to the Same Size\n",
        "xception_model = Xception(weights = \"imagenet\", include_top=False, input_tensor=inputlayer)\n",
        "\n",
        "conv1 = xception_model.output\n",
        "conv1 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(conv1)\n",
        "#conv1 = MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\"valid\")(conv1)\n",
        "\n",
        "\n",
        "resNet_model = ResNet50V2(weights = \"imagenet\", include_top=False, input_tensor=inputlayer)\n",
        "# for layers in resNet_model.layers[:-5]:\n",
        "#   layers.trainable = False\n",
        "conv2 = resNet_model.output\n",
        "conv2 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(conv2)\n",
        "#conv2 = MaxPooling2D(pool_size=(2,2), strides=(1,1), padding=\"valid\")(conv2)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenating the Base Models' Convolutions\n",
        "concat1 = tf.concat([conv1, conv2], 3)\n",
        "#concat1 = Flatten()(concat1)\n",
        "\n",
        "\n",
        "# Convolutional and Pooling Layers to Base Models' Convolutions\n",
        "\n",
        "# Xception Model\n",
        "conv1 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "conv1 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "conv1 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"same\", strides=1)(conv1)\n",
        "conv1 = BatchNormalization()(conv1)\n",
        "\n",
        "# ResNet Model\n",
        "conv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "conv2 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "conv2 = Conv2D(2048, (1, 1), activation=\"relu\", padding=\"same\", strides=1)(conv2)\n",
        "conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "# Concatenating the new Convolutions\n",
        "concat2 = tf.concat([conv1, conv2], 3)\n",
        "#concat2 = Flatten()(concat2)\n",
        "\n",
        "\n",
        "concat3 = tf.concat([concat1, concat2], 3)\n",
        "\n",
        "\n",
        "#concat3 = tf.reshape(xceptionConv, shape=[15, 15, 2048])\n",
        "concat3 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')(concat3)\n",
        "print(concat3)\n",
        "concat3 = Conv2D(4096, (1, 1), activation=\"relu\", padding=\"valid\", strides=1)(concat3)\n",
        "\n",
        "print(concat3)\n",
        "concat3 = Flatten()(concat3)\n",
        "print(concat3)\n",
        "\n",
        "\n",
        "\n",
        "fc1 = Dense(2056, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(concat3)\n",
        "fc1 = Dropout(0.05)(fc1)\n",
        "fc1 = BatchNormalization()(fc1)\n",
        "\n",
        "fc2 = Dense(2056, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc1)\n",
        "fc2 = Dropout(0.05)(fc2)\n",
        "fc2 = BatchNormalization()(fc2)\n",
        "\n",
        "fc3 = Dense(1024, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc2)\n",
        "fc3 = Dropout(0.05)(fc3)\n",
        "fc3 = BatchNormalization()(fc3)\n",
        "\n",
        "fc4 = Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=0.05, l2=0.05))(fc3)\n",
        "fc4 = Dropout(0.05)(fc4)\n",
        "fc4 = BatchNormalization()(fc4)\n",
        "\n",
        "\n",
        "\n",
        "preds = Dense(19, activation=\"softmax\")(fc4)\n",
        "\n",
        "model = Model(inputs = inputlayer, outputs = preds)\n",
        "\n",
        "\n",
        "# Freeze Pretrained Models\n",
        "for layers in model.layers[:-42]:\n",
        "   layers.trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWHGjF1UOxQL",
        "outputId": "288c7174-aec0-4bcf-ccbb-448a745d7da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py:1884: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 184s 11s/step - loss: 2.2807 - categorical_accuracy: 0.5946 - auc_1: 0.9231 - mean_average_precision_metric_1: 0.7199 - f1_score: 0.4452\n",
            "['loss', 'categorical_accuracy', 'auc_1', 'mean_average_precision_metric_1', 'f1_score']\n",
            "[<keras.metrics.base_metric.Mean object at 0x7fb0f6319220>, <keras.metrics.metrics.CategoricalAccuracy object at 0x7fb0f6521ee0>, <keras.metrics.metrics.AUC object at 0x7fb0f6a33ee0>, <tensorflow_ranking.python.keras.metrics.MeanAveragePrecisionMetric object at 0x7fb0f6317490>, <tensorflow_addons.metrics.f_scores.F1Score object at 0x7fb0f6317c70>]\n"
          ]
        }
      ],
      "source": [
        "#@title Default title text\n",
        "# config = tf.compat.v1.ConfigProto()\n",
        "# config.gpu_options.allow_growth=True\n",
        "# sess = tf.compat.v1.Session(config=config)\n",
        "# Train and run model\n",
        "\n",
        "'''\n",
        "Importance Level    Hyperparameters\n",
        "\n",
        "First               Learning Rate Alpha\n",
        "\n",
        "Second              preprocessing\n",
        "                    mini-batch size\n",
        "                    number of hidden units/layers\n",
        "\n",
        "Third               Adam beta1, beta2 (Between 1-0, higher means faster learning)\n",
        "                    Dropout and L1/L2 regulrization\n",
        "'''\n",
        "epochs = 10\n",
        "optimizer = Adam(learning_rate=0.000000003,\n",
        "                 beta_1=0.9,\n",
        "                 beta_2=0.999,)\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics = [\n",
        "                                                                                   tf.keras.metrics.CategoricalAccuracy(),\n",
        "                                                                                   tf.keras.metrics.AUC(),\n",
        "                                                                                   tfr.keras.metrics.MeanAveragePrecisionMetric(),\n",
        "                                                                                   tfa.metrics.F1Score(num_classes=19)])\n",
        "\n",
        "\n",
        "# Last train and load weights\n",
        "# checkpoint_path = \"/content/drive/MyDrive/training_1/cp-0007.ckpt\"\n",
        "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "\n",
        "# !ls /content/drive/MyDrive/training_1\n",
        "# latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "# print(latest)\n",
        "model.load_weights(\"/content/drive/MyDrive/training_6/cp.hdf5\")\n",
        "\n",
        "# New train/weights\n",
        "filepath = \"/content/drive/MyDrive/training_6/cp.hdf5\"\n",
        "#new_dir = os.path.dirname(new_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "#epoch_size = 128\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath,\n",
        "    verbose=1,\n",
        "    monitor='val_Categorical_Accuracy',\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    )\n",
        "\n",
        "# Fit model\n",
        "# model.fit(training_set,\n",
        "#           validation_data = validation_set,\n",
        "#           epochs = 1,\n",
        "#           #epochs = epochs,\n",
        "#           callbacks=[cp_callback]\n",
        "#           )\n",
        "\n",
        "#Evaluate model\n",
        "model.evaluate(validation_set,\n",
        "\n",
        "          #epochs = 1,\n",
        "          #epochs = epochs,\n",
        "          #callbacks=[cp_callback]\n",
        "          )\n",
        "print(model.metrics_names)\n",
        "print(model.metrics)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}